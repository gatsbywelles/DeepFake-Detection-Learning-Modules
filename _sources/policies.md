# Module 1: Policies

## What is Deepfake? 
The term “deepfake” blends the ideas of deep learning and fabricated media. It refers to the use of artificial intelligence to alter or generate audio and video content in a way that makes it appear genuine, even though it’s entirely constructed (Busacca & Monaca, 2023). These techniques can be used to digitally overlay one person’s face onto another’s in a video, or to replicate someone’s voice with convincing accuracy.

Deepfake technology relies on neural networks trained on large volumes of data. These systems learn to imitate facial movements, speech patterns, and subtle expressions. Typically, footage of two individuals is fed into the algorithm, which then learns how to convincingly swap one face for another. Through advanced image mapping and pattern recognition, the software generates a new video that falsely portrays someone saying or doing things they never actually did (Chadha et al., 2021).

## Types of Deepfake

### Video-Based Deepfake
+ Face-Swapping
  + This technique involves replacing the face of an individual in a video with that of another person. It’s commonly applied in film production, such as substituting a stunt performer’s face with that of the lead actor to maintain continuity and realism (Chadha et al., 2021).

+ Face-Morphing
  + Face-morphing gradually transforms one person’s facial features into another’s in a smooth, seamless transition. This method is often used in gaming environments, allowing players to upload their own photos so that in-game characters adopt their likeness—enhancing immersion and personalization (Chadha et al., 2021).
 
## Risks of Deepfake
### Misinformation
Due to the affordability and widespread availability of AI tools, deepfakes pose a significant threat in spreading false information. They can be leveraged to influence political campaigns, damage reputations, and incite public unrest—particularly in an era where political activities are heavily digitized (Chauhan, 2024).

### Ethical Concerns
Key ethical issues include matters of consent, the erosion of truth, and the impact on media consumption. Deepfakes are often used to produce non‑consensual pornography or other harmful material. They can also facilitate identity theft and fraud by enabling the impersonation of individuals for financial gain or other criminal purposes (Chauhan, 2024).

### Cyberbullying
Deepfake technology has the potential to transform bullying dynamics. While traditional bullying typically involves direct physical confrontation or online harassment through messages and posts, deepfakes introduce new forms of abuse (Alexander, 2025). Research indicates that global rates of cyberbullying victimization among adolescents range from 13.99% to 57.5%, with notable increases over the past decade as digital technologies have become more accessible and advanced (Zhu et al., 2021).

### Financial Scams
Malicious actors can create deepfake audio messages that impersonate individuals in authority, such as CEOs or bank representatives, to trick employees or clients into transferring funds or divulging sensitive information. This type of fraud exploits the trust typically placed in voice communication (Hery et al., 2024). 

### Privacy Violations
Deepfake technology enables individuals to digitally recreate someone’s face, voice, or gestures in fabricated content that appears convincingly real. For those targeted, this can be highly distressing. Imagine discovering your image has been inserted into explicit or misleading videos without your consent. The emotional toll can be severe, affecting mental wellbeing and personal safety. One of the most harmful applications involves the creation and circulation of non-consensual adult material, often shared without the victim’s knowledge. This kind of exploitation should never be a concern for anyone (Chaturvedi et al., 2025).

### Challenges for the Legal System
The impact of deepfakes extends beyond personal harm and poses serious risks to the justice system. Digital evidence, once considered reliable, can now be manipulated with ease. Consider a situation where a video showing a suspect confessing to a crime is later revealed to be artificially generated. Or a fabricated recording is used to falsely accuse someone of wrongdoing. These scenarios raise major concerns for law enforcement and legal professionals who depend on the authenticity of audio and video evidence. As deepfake tools become more sophisticated, maintaining trust in digital records becomes increasingly difficult (Chaturvedi et al., 2025).

## Legal and Ethical Guidelines 
| Scenario | Consent in AI Media | Data Protection & Privacy | Ethical Consideration | Escalation & Enforcements |
| -------- | ------------------- | ------------------------- | ----------------------|---------------------------|
|Using a staff member’s face in a training deepfake video | Must obtain explicit written consent (release form or signed agreement). No implied consent is valid. | Must comply with the Privacy Act 1988 (Cth) and Australian Privacy Principles (APPs) on handling personal information. | Respect autonomy and dignity; avoid misuse of identity (aligns with eSafety’s Safety by Design principles). | Report unconsented use to HR and Privacy Officer. Breach may result in disciplinary action under organizational policy. |
|Sharing AI-generated media on social platform | Must disclose that content is synthetic or altered. Consent needed if a person’s likeness is used. | Content must not expose personal information without authorization. | Transparency is required; manipulated media should be clearly labeled |Escalate to Comms/Legal before external posting. Misuse can result in reputational and legal consequences. |
|AI-simulated CEO voice used in a phishing scenario | Prohibited unless simulated under controlled training with consent of the person represented. | Audio samples fall under “personal information” under Privacy Act (APP 3 – collection). | Misuse undermines trust and can constitute fraud. | Report immediately to Security Operations. Treated as a cyber incident under Notifiable Data Breach Scheme. |
|Student-generated deepfake for an assignment without disclosure | Must have consent from any depicted individual. | Use of peer likeness without consent breaches Privacy Act. | Academic integrity requires transparency—unlabeled AI use = misconduct. | Report to Academic Integrity Office. Can lead to disciplinary hearings |
|Distribution of manipulated political or social media content | Consent from depicted persons mandatory. | If personal information is involved, APP 11 (security of personal info) applies. | Raises ethical concerns around misinformation, democracy, and trust (Chesney & Citron, 2019). | Escalate to Legal and report to eSafety Commissioner if harmful/disinformation. |
|Corporate training scenario: AI-generated audio of a manager giving safety instructions |Consent required from the manager before generating audio likeness. Synthetic media must be disclosed to staff. |Voice recordings are considered personal information; must be securely stored per APP 11. | Misrepresentation may confuse staff if not labeled as synthetic. | Escalate to HR/Training if unconsented use is found; re-issue corrected material. |
|University lecture recording altered with AI to mimic a professor’s voice | Written consent from the lecturer required before AI editing or replication. | Altering or sharing lecture audio without consent may breach the Privacy Act and copyright. | Ethical risk: undermines academic integrity and authenticity of teaching. |Report to Faculty Head + IT Security. May trigger academic misconduct proceedings. |

## Example Case Studies 
![Arup Scam](_static/arup.jpg)

An employee at British multinational Arup was duped into sending $39 million to fraudsters after the company's CEO and other staff members were impersonated in a video call. 

The professional in question was initially contacted by Arup's UK office with an email stressing the need for a secret transaction. The individual suspected it to be a phishing email, but their doubts were removed after the video call because of the extremely realistic impersonations. 